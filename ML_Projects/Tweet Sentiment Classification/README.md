# üìù Tweet Sentiment Classification 

This project performs sentiment classification on Twitter messages using classical machine-learning methods and text-vectorization techniques. The workflow includes exploratory data analysis, text preprocessing, feature extraction (Bag-of-Words and TF-IDF), model training, evaluation, error analysis, and interpretation of feature importance.


## Dataset
Each record contains:

- **textID** ‚Äî unique identifier  
- **text** ‚Äî full tweet text  
- **selected_text** ‚Äî span of text expressing sentiment  
- **sentiment** ‚Äî target label: *negative*, *neutral*, *positive*  

Total samples: **27,480 tweets**.



## Exploratory Data Analysis 

### ‚úî Class distribution
Tweets are slightly imbalanced:
- **neutral** ‚Äî 11,117  
- **positive** ‚Äî 8,582  
- **negative** ‚Äî 7,781  

The classifier tends to predict **neutral** more often.

### ‚úî Tweet length analysis
- Avg. length ‚âà **68 characters**
- 50% of tweets shorter than **64** chars  
- 75% shorter than **97** chars  

No need for long-sequence models (e.g., LSTMs).


## Text Preprocessing

Steps applied:

- lowercasing  
- removing punctuation and non-alphabetic characters  
- tokenization  
- stopword removal  
- stemming (SnowballStemmer)  

Result is stored in `clean_text`.



## Bag-of-Words Vectorization 

- Full vocabulary: **22,746 unique terms**
- Most words appear only **1‚Äì2 times** ‚Üí noisy, low signal  
- Corpus coverage analysis shows:
  - **Top 5,000 words** achieve best signal-to-noise ratio  

Final representation: **5000-feature CountVectorizer**

### ‚úî Models trained:
- Logistic Regression  
- Decision Tree  
- Random Forest  
- XGBoost  

Best BoW model: **XGBoost (Accuracy ‚âà 0.694)**  
Neutral & positive classified best; negative often confused with neutral.



## Word-Importance Analysis

### ‚úî Logistic Regression coefficients
- **negative**: sad, suck, bore, hate, fail  
- **positive**: great, amaz, enjoy, thank, love  
- **neutral**: context-free tokens (e.g., how, indoor, jst)

The model correctly captures emotional vocabulary.

### ‚úî XGBoost feature importance
Top features include sentiment-heavy tokens (e.g., *sad*, *love*, *miss*).



## TF-IDF Vectorization

- TF-IDF reduces influence of frequent generic words
- Strengthens meaningful sentiment tokens
- Still using **5000 features**

### ‚úî Models trained again:
- Logistic Regression  
- Decision Tree  
- Random Forest  
- XGBoost  

Best TF-IDF model: **Random Forest (Accuracy ‚âà 0.70)**  
Shows improved generalization and fewer neutral/negative confusions.

**TF-IDF outperforms BoW** and is recommended as the final feature representation.



## Error Analysis

Confusion matrix shows:

- **Most errors occur between:**
  - negative ‚Üî neutral (482 cases)
  - positive ‚Üî neutral (388 cases)

Reason:  
Tweets with weak emotional cues are difficult even for humans.



## Possible Improvements

- Use **bigrams/trigrams** in TF-IDF  
- Replace stemming with **lemmatization**  
- Use advanced embeddings:
  - Word2Vec / GloVe  
  - Transformers (BERT, DistilBERT)  
- Apply class-balancing techniques  
- Hyperparameter optimization with Bayesian search  


## Tools & Libraries

- **Python**: Pandas, NumPy, re, NLTK  
- **Vectorization**: CountVectorizer, TfidfVectorizer  
- **Models**: Logistic Regression, Decision Tree, Random Forest, XGBoost  
- **Evaluation**: accuracy, precision, recall, F1, confusion matrix  
- **Visualization**: Matplotlib, Seaborn  

